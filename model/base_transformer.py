# -*- coding: utf-8 -*-
"""base_transformer

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pxRb4dtV2ZLkzoFMRaq44sObxr4k-2-i
"""

import math


base_dir = '/content/drive/MyDrive/sarcasmdetection/'

# Commented out IPython magic to ensure Python compatibility.
# %pip install fasttext
import fasttext.util
fasttext.util.download_model('en',if_exists='ignore') #design
ft = fasttext.load_model('cc.en.300.bin')

# Commented out IPython magic to ensure Python compatibility.
# %pip install torch

import torch
import json
import pandas as pd
import torch
from torch.utils.data import Dataset, RandomSampler, DataLoader
import matplotlib.pyplot as plt
import re
import nltk
import torch.nn as nn
from sklearn.model_selection import train_test_split
import numpy as np
import torch.nn.functional as F
import ast

import os

os.environ['HF_TOKEN'] = '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'

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

emb_file = base_dir + 'emb_lookup.txt'
dict_file = base_dir + 'int_dict.txt'

with open(emb_file, 'r') as file :
  emb_lookup = json.load(file)

with open(dict_file, 'r') as file :
  vocab_set = json.load(file)

print(len(emb_lookup.keys()))

print(emb_lookup.keys())

class CustomTextDataset(Dataset) :
    # convert words into numbers -> corresponding to the

    def __init__(self, df, transform=None, target_transform=None, vocab_set=None) :
        self.df = df
        self.transform = transform
        self.target_transform = target_transform
        n = self.df.shape[0]
        self.max_length = max(len(df.iloc[i, 0]) for i in range(n))
        self.vocab_size = len(vocab_set) if vocab_set != None else 0

    def __len__(self):
        return self.df.shape[0]

    def __getitem__(self, idx) :
          text_sample = self.df.iloc[idx, 0]#list of words
          label = self.df.iloc[idx,1]


          #array of words
          # for word in text_sample  :

            # try :
            #   new_sample.append(vocab_set[word])
            # except Exception as e :
            #   print(e)
            #   print(word)
          # print(type(text_sample))
          # for word in text_sample :
          #   print(word)

          text_sample = torch.tensor([vocab_set[word] for word in text_sample], dtype=torch.int)
          text_sample = F.pad(text_sample, (0, max(self.max_length - len(text_sample), 0)), value=0)


          return text_sample, idx , torch.tensor(label, dtype=torch.int)

BATCH_SIZE = 100
SHUFFLE = True

#load df

df_reddit = pd.read_csv(base_dir + 'df_reddit.csv')
df_twitter = pd.read_csv(base_dir + 'df_twitter.csv')
df_iac = pd.read_csv(base_dir + 'df_iac.csv')

df_reddit['text'] = df_reddit['text'].apply(lambda x: ast.literal_eval(x))
df_twitter['text'] = df_twitter['text'].apply(lambda x: ast.literal_eval(x))
df_iac['text'] = df_iac['text'].apply(lambda x: ast.literal_eval(x))


print(df_reddit.columns)
print(df_twitter.columns)
print(df_iac.columns)

print(df_reddit)
print(df_twitter)
print(df_iac)

sarc = 0
not_sarc = 1

not_sarc = np.sum(df_reddit['label'] == 0)
sarc = np.sum(df_reddit['label']==1)
print(sarc, not_sarc)
print(df_reddit.shape)

df = pd.concat([df_reddit, df_twitter, df_iac], ignore_index=True)
# df["text"] = df["text"].tolist()


# df = df
print(df.shape)


train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)

train_dataset = CustomTextDataset(train_df) #train topic modelling on train set
val_dataset = CustomTextDataset(val_df)
max_length = train_dataset.max_length
print("max_length",max_length)

train_sampler = RandomSampler(train_dataset)
val_sampler   = RandomSampler(val_dataset)

#LDA model
from gensim import corpora
from gensim.models import LdaModel
from gensim.test.utils import common_texts
from gensim.utils import simple_preprocess

# corpus = [simple_preprocess(doc) for doc in common_texts]
vocab = [key for key in vocab_set.keys()]
vocab.remove("PAD")

#train on non-padded sequence
dictionary = corpora.Dictionary(common_texts)
dictionary.add_documents([vocab])

corpus = []
for sentence in df["text"] :
  # print(sentence)
  corpus.append(sentence)


corpus_bow = [dictionary.doc2bow(doc) for doc in corpus]


lda_model = LdaModel(corpus_bow, num_topics=5, id2word=dictionary) #bag of words LDA or contextual LDA


topic_feature_vectors = []

lda_set = { }
co = 0

for idx, sample_bow in enumerate(corpus_bow):
    topic_distribution = lda_model.get_document_topics(sample_bow, minimum_probability=0)
    feature_vector = [score for _, score in topic_distribution]
    topic_feature_vectors.append(feature_vector)
    print(corpus[co])
    print(sample_bow)
    print(feature_vector)
    co+=1

    lda_set[idx] = feature_vector

lda_set = torch.stack([torch.tensor(lda_set[idx]).to(device) for idx in sorted(lda_set.keys())]).to(device)
#verify lda_set dimension
print(lda_set.size())

train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)
val_iterator   = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)

for x, indices, y in train_iterator:
    print(f'x: {x.shape}')
    print(f'y: {y.shape}')
    print(indices)

    break

#generate pretrained embedding matrix

vocab_size = len(vocab_set)
print(emb_lookup.keys())
if vocab_size == len(emb_lookup):
  print("equalll")
pretrain_emb_matrix = [ ]
for i in range(2,vocab_size) :
  emb_vector = emb_lookup[str(i)]
  pretrain_emb_matrix.append(emb_vector)

print("vocab size", vocab_size)

# Commented out IPython magic to ensure Python compatibility.
# %pip install transformers
from transformers import BertModel, BertTokenizer, BertConfig,AutoModelWithLMHead

from transformers import DistilBertModel

STRIDE = 2
class SarcasmModel(nn.Module) :

  def __init__(self, vocab_size, hidden_dim, cnn_dim, bert_config, static_emb_dim=300, bert_alone=False) :
    """
    vocab_size (int) : size of vocab
    embedding_dim (int) : embedding dimension
    cnn_dim (Dict(String, List[int])) : list of CNN layer dimensions for "conv" & "pool"
    """
    super(SarcasmModel, self).__init__()

    #frozen embedding layer
    self.embedding = nn.Embedding(vocab_size, static_emb_dim) #static embeddings
    self.conv_layers = nn.ModuleList()
    self.cnn_pool_layers = nn.ModuleList()
    self.cnn_relu = nn.ReLU()
    #CNN ---------


    l_i = max_length
    n_i = 1

    cnn_output_dim = static_emb_dim
    num_layers = len(cnn_dim["conv"]["k"])
    for i in range(num_layers) :
      if i == 0 :
        in_channels = static_emb_dim
      else :
        in_channels = cnn_dim["conv"]["out"][i-1]

      #cnn params
      out_channels = cnn_dim["conv"]["out"][i]
      kernel_size = cnn_dim["conv"]["k"][i]

      conv = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=STRIDE, padding=0)
      val = (((l_i) - kernel_size) / STRIDE) + 1
      l_i = math.floor(val)
      n_i = out_channels
      # print("dim conv", l_i, n_i)

      pool_size = cnn_dim["pool"]["k"][i]
      pool = nn.MaxPool1d(kernel_size=pool_size, stride=STRIDE)

      val = ((l_i - pool_size) / STRIDE) + 1
      l_i = math.floor(val)
      # print("dim pool", l_i, n_i)

      self.conv_layers.append(conv)
      self.cnn_pool_layers.append(pool)
    # print("cnn out dim", l_i,  n_i)
    #------------
    #Freeze layer
    # self.transformer = DistilBertModel.from_pretrained('distilbert-base-uncased')
    self.transformer = AutoModelWithLMHead.from_pretrained("mrm8488/t5-base-finetuned-sarcasm-twitter")
    for param in self.transformer.parameters():
      param.requires_grad = False

    # Update the max_position_embeddings parameter
    cnn_out_dim = l_i * n_i
    bert_out_dim = 768
    lda_out_dim = 5
    total_out_dim = cnn_out_dim  + lda_out_dim
    print("total output_dim", total_out_dim)
    self.output_layer = nn.Linear(total_out_dim, 1)
    self.sigmoid_layer = nn.Sigmoid()
    self.bert_alone = bert_alone
    #import LDA feature vector & concatenate

  def forward(self, x, indices):

    #x : (batch_size, max_length)
    #indices : (batch_size)
    embedded_x = self.embedding(x) #obtain static embeddings
    print("shape", x.size())
    print("embedded x", embedded_x.size())

    bert_x = x
    if x.size(1) > 511 :
      bert_x = x[:,:511]

    # input_ids = x
    # attention_mask = (bert_x!=0).long()
    # bert_output = self.transformer(bert_x, attention_mask=attention_mask)[0]
    # pooled_output = bert_output[:, 0, :] #(batch size, hidden_size)

    # pre_output = pooled_output
    #apply CNN & LDA model
    print("input shape", x.size())

    if not self.bert_alone :
      cnn_input = embedded_x.permute(0, 2, 1) #(batch size, length, emb dim)
      for conv_layer, pool_layer in zip(self.conv_layers, self.cnn_pool_layers):
            conv_output = conv_layer(cnn_input) #static embeddings on

            conv_output = self.cnn_relu(conv_output)
            print("conv_output after relu", conv_output.size())
            cnn_pool_output = pool_layer(conv_output)
            print("conv after pool", cnn_pool_output.size())
            cnn_input = cnn_pool_output


      #obtain LDA
      cnn_flatten = torch.flatten(cnn_input, 1)
      # cnn_input.view(cnn_input.size(0), -1)
      lda_tensor = lda_set[indices] #(batch_size, feature vector)
      concat_output = torch.cat((cnn_flatten, lda_tensor), dim=1) #(batch_size, feature vector)
      # print("cnn_flatten", cnn_flatten.size())
      # print("lda tensor", lda_tensor.size())
      # print("bert pre_output", pre_output.size())
      # print("cnn lda", concat_output.size())
      # pre_output = torch.cat((pre_output, concat_output), dim=1) #(batch_size, feature vector)

    pre_output = concat_output
    fc_output = self.output_layer(pre_output)
    output = self.sigmoid_layer(fc_output)
    return output

for x, indices, y in train_iterator:
    print(f'x: {x.shape}')
    print(f'y: {y.shape}')
    print(indices)

import torch.optim as optim


cnn_dim = {
  "conv" : {
      "k" : [3,3,3],
        "out": [64,32,16],
      # "stride" : [], #default stride = 2
      # "padding" : [], #same padding
  },
  "pool" : {
      "k" : [2,2,2]
    }
  }

config = BertConfig(
    num_hidden_layers=6,  #adjust number of hidden layers - whilst extending to manual features
    hidden_size=512,
    num_attention_heads=10
)
num_epochs = 20
# vocab_size, hidden_dim, cnn_dim, bert_config, static_emb_dim=300, bert_alone=False
model = SarcasmModel(vocab_size, 512, cnn_dim, config)
model = model.to(device)

optimizer = optim.Adam(model.parameters(), lr=1e-5)

# Define your loss function
criterion = nn.BCELoss()

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for x, indices, y in train_iterator:

        optimizer.zero_grad()

        print(x, indices, y)
        x = x.to(device)
        y = y.to(device)
        outputs = model(x, indices).float()
        y = y.float().unsqueeze(1)
        loss = criterion(outputs, y)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch {epoch+1}, Loss: {running_loss/len(train_iterator)}")

model.eval()
total_correct = 0
total_samples = 0

for x ,indices, labels in val_iterator:
    x = x.to(device)
    labels = labels.to(device)
    indices = indices.to(device)
    outputs = model(x, indices)
    predicted = outputs > 0.5  # Assuming threshold of 0.5 for binary classification
    total_correct += (predicted == labels).sum().item()
    total_samples += labels.size(0)

accuracy = total_correct / total_samples
print(f"Accuracy: {accuracy}")

