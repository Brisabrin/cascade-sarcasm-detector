# -*- coding: utf-8 -*-
"""benchmarktest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hgo0WKIdGFeJIp8YGhPi3tYhz3WoRzR2
"""


base_dir = '/content/drive/MyDrive/sarcasmdetection/'

# %pip install torch
# %pip install pandas
# %pip install numpy
# %pip install matplotlib
# %pip install nltk
# %pip install spacy

import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

import os

os.environ['HF_TOKEN'] = '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'
os.environ['COLAB_APP_IOPUB_DATA_RATE_LIMIT'] = '1000000000'

"""Multi-dataset import : Reddit, Twitter, *IAC*"""

import os
import torch
import json
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset
import matplotlib.pyplot as plt
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
import gensim.downloader as api
import spacy


if 'en_core_web_sm' not in spacy.util.get_installed_models():
    spacy.cli.download('en_core_web_sm')
nlp = spacy.load("en_core_web_sm")

base_dir = '/content/drive/MyDrive/sarcasmdetection/'


#processs each dataset separately first -> gain insight

#get dictionary for all words in dataframe

#path
reddit = {
 'test' : base_dir + 'reddit/sarcasm_detection_shared_task_reddit_testing.jsonl',
 'train' : base_dir + 'reddit/sarcasm_detection_shared_task_reddit_training.jsonl'
}

twitter = {
    'train' : base_dir + 'twitter/sarcasm_detection_shared_task_twitter_training.jsonl',
    'test' : base_dir + 'twitter/sarcasm_detection_shared_task_twitter_testing.jsonl'
}

iac = [base_dir + 'sarcasm_v2/GEN-sarc-notsarc.csv', base_dir + 'sarcasm_v2/HYP-sarc-notsarc.csv',
       base_dir + 'sarcasm_v2/RQ-sarc-notsarc.csv']


#dataset
df_reddit = pd.DataFrame(columns=['text', 'label'])
df_twitter = pd.DataFrame(columns=['text', 'label'])
df_iac = pd.DataFrame(columns=['text', 'label'])

# Process reddit dataset
for file_name in [reddit['train'], reddit['test']]:
    with open(file_name, 'r') as file:
        for line in file:
            obj = json.loads(line)
            y = obj['label']
            context = " ".join(obj['context'])
            resp = obj['response']
            s = context + resp
            df_reddit = pd.concat([df_reddit, pd.DataFrame({'text': [s], 'label': [y]})], ignore_index=True)

# Process twitter dataset
for file_name in [twitter['train'], twitter['test']]:
    with open(file_name, 'r') as file:
        for line in file:
            obj = json.loads(line)
            y = obj['label']
            context = " ".join(obj['context'])
            resp = obj['response']
            s = context + resp
            df_twitter = pd.concat([df_twitter, pd.DataFrame({'text': [s], 'label': [y]})], ignore_index=True)

# Process iac dataset
for file_name in iac:
    df = pd.read_csv(file_name)
    df = df.iloc[:, [0, 2]]
    df.columns = ['label','text']
    df_iac = pd.concat([df_iac, df])
    df_iac = df_iac[['text','label']]


df_reddit['label'] = df_reddit['label'].replace({'NOT_SARCASM': 0, 'SARCASM': 1})
df_twitter['label'] = df_twitter['label'].replace({'NOT_SARCASM': 0, 'SARCASM': 1})
df_iac['label'] = df_iac['label'].replace({'notsarc': 0, 'sarc': 1})

"""Preprocessing stage"""

from transformers import DistilBertTokenizer

#create dictionary from words
def stop_word_remove(l) :
    stop_words = set(stopwords.words('english'))
    return [elem for elem in l if elem not in stop_words]

lemmatizer = WordNetLemmatizer()

def lemmatize_tokens(tokens):
    # convert POS tag to WordNet format
    def get_wordnet_pos(word):
        tag = nltk.pos_tag([word])[0][1][0].upper()
        tag_dict = {"J": wordnet.ADJ,
                    "N": wordnet.NOUN,
                    "V": wordnet.VERB,
                    "R": wordnet.ADV}
        return tag_dict.get(tag, wordnet.NOUN)

    lemmas = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]

    return lemmas

#use BERT tokenizer instead
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

#encoded_input = tokenizer.encode(text, truncation=True, max_length=512)
def bert_tokenize(text) :

    return [token for token in tokenizer.tokenize(text)]

def preprocess(df) :

    #lowercase
    df['text'] = df['text'].apply(lambda x: x.lower() if isinstance(x, str) else x)
    #url & non-white space removal
    url_pattern = re.compile(r'https?://\S+')

    df['text'] = df['text'].apply(lambda x: url_pattern.sub('',x))
    #remove non white space
    df['text'] = df['text'].replace(to_replace=r'[^\w\s]', value='', regex=True)
    #digits
    df['text'] = df['text'].replace(to_replace=r'\d', value='', regex=True)

    # #tokenization
    # df['text'] = df['text'].apply(lambda x : bert_tokenize(x))

    return df


df_reddit = preprocess(df_reddit)
df_twitter = preprocess(df_twitter)
df_iac = preprocess(df_iac)


print(df_reddit)
print(df_twitter)
print(df_iac)
#string data

df_reddit = pd.read_csv(base_dir + 'df_reddit.csv')
df_twitter = pd.read_csv(base_dir + 'df_twitter.csv')
df_iac = pd.read_csv(base_dir + 'df_iac.csv')

"""Tokenize input"""

from transformers import DistilBertTokenizer
import torch
from torch.utils.data import TensorDataset, DataLoader
import pandas as pd

SEQ_LENGTH = 512
#choose 1 dataset
df = df_iac
labels = df['label']
text_column = df['text']
tokenized_text = [tokenizer.encode_plus(text, add_special_tokens=True, truncation=True, max_length=SEQ_LENGTH, padding='max_length', return_tensors='pt') for text in text_column]
input_ids = torch.cat([token['input_ids'] for token in tokenized_text], dim=0)
attention_masks = torch.cat([token['attention_mask'] for token in tokenized_text], dim=0)

labels = torch.tensor(labels)

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')


dataset = TensorDataset(input_ids, attention_masks, labels)

# Create DataLoader
batch_size = 30
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

"""Train Test IAC"""

from sklearn.model_selection import train_test_split

# Split dataset into train and test sets
train_texts, test_texts, train_labels, test_labels = train_test_split(
    text_column, labels, test_size=0.2, random_state=42)

# Tokenize train and test texts
train_tokenized_texts = [tokenizer.encode_plus(text, add_special_tokens=True, truncation=True, max_length=SEQ_LENGTH, padding='max_length', return_tensors='pt') for text in train_texts]
test_tokenized_texts = [tokenizer.encode_plus(text, add_special_tokens=True, truncation=True, max_length=SEQ_LENGTH, padding='max_length', return_tensors='pt') for text in test_texts]

# Concatenate input_ids and attention_masks for train and test sets
train_input_ids = torch.cat([token['input_ids'] for token in train_tokenized_texts], dim=0)
train_attention_masks = torch.cat([token['attention_mask'] for token in train_tokenized_texts], dim=0)

test_input_ids = torch.cat([token['input_ids'] for token in test_tokenized_texts], dim=0)
test_attention_masks = torch.cat([token['attention_mask'] for token in test_tokenized_texts], dim=0)

# Convert labels to tensors
train_labels = torch.tensor(train_labels)
test_labels = torch.tensor(test_labels)

# Create TensorDataset and DataLoader for train and test sets
train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)
test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)

# Create DataLoader for train and test sets
batch_size = 30
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # No need to shuffle for testing

from sklearn.metrics import accuracy_score
from transformers import DistilBertForSequenceClassification, AdamW
from tqdm import tqdm


model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)

optimizer = AdamW(model.parameters(), lr=5e-5)

criterion = torch.nn.CrossEntropyLoss()

model = model.to(device)
model.train()

num_epochs = 3
for epoch in range(num_epochs):
    # Training phase
    model.train()  # Set model to training mode
    running_loss = 0.0
    total_correct = 0
    total_samples = 0
    for batch in tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{num_epochs} (Training)"):
        input_ids, attention_masks, labels = batch

        optimizer.zero_grad()
        input_ids = input_ids.to(device)
        attention_masks = attention_masks.to(device)
        labels = labels.to(device)

        outputs = model(input_ids, attention_mask=attention_masks)
        logits = outputs.logits

        loss = criterion(logits, labels)

        loss.backward()

        optimizer.step()
        running_loss += loss.item()
        _, predicted = torch.max(logits, 1)
        total_correct += (predicted == labels).sum().item()
        total_samples += labels.size(0)

    epoch_loss = running_loss / len(train_dataloader)
    epoch_accuracy = total_correct / total_samples
    print(f'Training - Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}, Accuracy: {epoch_accuracy}')

# Testing phase
model.eval()  # Set model to evaluation mode
total_correct = 0
total_samples = 0
with torch.no_grad():  # No need to track gradients during testing
    for batch in tqdm(test_dataloader, desc=f"Testing"):
        input_ids, attention_masks, labels = batch

        input_ids = input_ids.to(device)
        attention_masks = attention_masks.to(device)
        labels = labels.to(device)

        outputs = model(input_ids, attention_mask=attention_masks)
        logits = outputs.logits

        _, predicted = torch.max(logits, 1)
        total_correct += (predicted == labels).sum().item()
        total_samples += labels.size(0)

test_accuracy = total_correct / total_samples
print(f'Testing Accuracy: {test_accuracy}')

from transformers import DistilBertForSequenceClassification, AdamW
from tqdm import tqdm

model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)

optimizer = AdamW(model.parameters(), lr=5e-5)

criterion = torch.nn.CrossEntropyLoss()

model = model.to(device)
model.train()

num_epochs = 3

for epoch in range(num_epochs):
    running_loss = 0.0

    for batch in tqdm(dataloader, desc=f"Epoch {epoch+1}/{num_epochs}"):
        input_ids, attention_masks, labels = batch

        optimizer.zero_grad()

        input_ids = input_ids.to(device)
        attention_masks = attention_masks.to(device)
        labels = labels.to(device)
        outputs = model(input_ids, attention_mask=attention_masks)
        logits = outputs.logits

        loss = criterion(logits, labels)

        loss.backward()

        optimizer.step()

        running_loss += loss.item()

    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(dataloader)}')

"""Run with accuracy"""

from sklearn.metrics import accuracy_score

# Training loop
for epoch in range(num_epochs):
    # Initialize variables for tracking loss and accuracy
    running_loss = 0.0
    total_correct = 0
    total_samples = 0
    for batch in tqdm(dataloader, desc=f"Epoch {epoch+1}/{num_epochs}"):
        input_ids, attention_masks, labels = batch

        optimizer.zero_grad()
        input_ids = input_ids.to(device)
        attention_masks = attention_masks.to(device)
        labels = labels.to(device)

        outputs = model(input_ids, attention_mask=attention_masks)
        logits = outputs.logits

        loss = criterion(logits, labels)

        loss.backward()

        optimizer.step()
        running_loss += loss.item()
        _, predicted = torch.max(logits, 1)
        total_correct += (predicted == labels).sum().item()
        total_samples += labels.size(0)

    epoch_loss = running_loss / len(dataloader)
    epoch_accuracy = total_correct / total_samples
    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}, Accuracy: {epoch_accuracy}')

# #save hugging face model
# path_name = 'pretrain_iac'
# model.save_pretrained(base_dir + path_name)

# #load model
# model_path = base_dir + 'pretrain_iac'
# model = DistilBertForSequenceClassification.from_pretrained(model_path, num_labels=2)

# # model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)
# # model.load_state_dict(torch.load('path_to_save_model.pth'))
# model = model.to(device)

"""Testing model

Twitter Dataset
"""

# Tokenize text in df_twitter
labels_twitter = df_twitter['label']
text_column_twitter = df_twitter['text']
tokenized_text_twitter = [tokenizer.encode_plus(text, add_special_tokens=True, truncation=True, max_length=SEQ_LENGTH, padding='max_length', return_tensors='pt') for text in text_column_twitter]
input_ids_twitter = torch.cat([token['input_ids'] for token in tokenized_text_twitter], dim=0)
attention_masks_twitter = torch.cat([token['attention_mask'] for token in tokenized_text_twitter], dim=0)

labels_twitter = torch.tensor(labels_twitter)

# Create DataLoader for df_twitter
dataset_twitter = TensorDataset(input_ids_twitter, attention_masks_twitter, labels_twitter)
dataloader_twitter = DataLoader(dataset_twitter, batch_size=batch_size, shuffle=False)  # No need to shuffle for evaluation

# Evaluate the model on df_twitter
model.eval()  # Set model to evaluation mode
total_correct_twitter = 0
total_samples_twitter = 0

model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)
model = model.to(device)
model.eval()
#train on original
with torch.no_grad():  # No need to track gradients during evaluation
    for batch in dataloader_twitter:
        # Unpack the batch
        input_ids, attention_masks, labels = batch

        # Move tensors to device
        input_ids = input_ids.to(device)
        attention_masks = attention_masks.to(device)
        labels = labels.to(device)

        # Forward pass
        outputs = model(input_ids, attention_mask=attention_masks)
        logits = outputs.logits

        # Predictions
        _, predicted = torch.max(logits, 1)

        # Update total samples and correct predictions
        total_correct_twitter += (predicted == labels).sum().item()
        total_samples_twitter += labels.size(0)

# Calculate accuracy
accuracy_twitter = total_correct_twitter / total_samples_twitter
print(f"Accuracy on df_twitter: {accuracy_twitter}")

"""Reddit Dataset"""

# Tokenize text in df_twitter
labels_reddit = df_reddit['label']
text_column_reddit = df_reddit['text']
tokenized_text_reddit = [tokenizer.encode_plus(text, add_special_tokens=True, truncation=True, max_length=SEQ_LENGTH, padding='max_length', return_tensors='pt') for text in text_column_reddit]
input_ids_reddit = torch.cat([token['input_ids'] for token in tokenized_text_reddit], dim=0)
attention_masks_reddit = torch.cat([token['attention_mask'] for token in tokenized_text_reddit], dim=0)

labels_reddit = torch.tensor(labels_reddit)

# Create DataLoader for df_twitter
dataset_reddit = TensorDataset(input_ids_reddit, attention_masks_reddit, labels_reddit)
dataloader_reddit = DataLoader(dataset_reddit, batch_size=batch_size, shuffle=False)  # No need to shuffle for evaluation

# Evaluate the model on df_twitter
model.eval()  # Set model to evaluation mode
total_correct_reddit = 0
total_samples_reddit = 0

model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)
model = model.to(device)
model.eval()
#train on original
with torch.no_grad():  # No need to track gradients during evaluation
    for batch in dataloader_reddit:

        input_ids, attention_masks, labels = batch

        # Move tensors to device
        input_ids = input_ids.to(device)
        attention_masks = attention_masks.to(device)
        labels = labels.to(device)

        # Forward pass
        outputs = model(input_ids, attention_mask=attention_masks)
        logits = outputs.logits

        # Predictions
        _, predicted = torch.max(logits, 1)

        # Update total samples and correct predictions
        total_correct_reddit += (predicted == labels).sum().item()
        total_samples_reddit += labels.size(0)

# Calculate accuracy
accuracy_reddit = total_correct_reddit / total_samples_reddit
print(f"Accuracy on df_reddit: {accuracy_reddit}")

